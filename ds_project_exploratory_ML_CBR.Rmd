---
title: "Data Science project"
author: "Carmen Rodriguez Cabrera"
date: "11/29/2021"
output:
  html_document: default
  pdf_document: default
---

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(knitr)
library(tidyverse)
library(caret)
library(randomForest)
#install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
```

# Data checks and data cleaning

```{r}
recipesdat<-read.csv(file = "recipes_final.csv")
str(recipesdat) # 1142 recipes


#Dataset is at the recipe level- so we can only use recipe level features

#We should assign each recipe an ID_number
recipesdat<-rowid_to_column(recipesdat, "recipe_ID")
head(recipesdat)
```


```{r}
#Outcome: rating
summary(recipesdat$rating) #7 missing rating and so we remove missing
recipesdat %>% filter(is.na(rating)) %>% dplyr::select(recipe, servings)
#recipesdat<-recipesdat %>% drop_na(rating) #1135 recipes

#table(recipesdat$rating)


```

# Data Quality Checks - Addressed 11/30/21

```{r eval=FALSE, include=TRUE}
####---------Features of interest-----------------####
#n_ratings, cook_time, prep_time, servings, n_ingredients, n_steps, cuisine(maybe)

#Cook Time and Prep time
#How do we distinguish hour and minutes from cook or prep time?-- ASK EMMA 
recipesdat %>% filter(prep_time < 10) # we found typos on the website

#Servings
summary(recipesdat$servings)
#16 missing this variable
#Max= 750--check, and minimum is 1

#recipes with very large serving sizes: 220 and 750
recipescheck<-recipesdat %>% filter(servings >60) %>% select(recipe)
#these are supposed to be 750 ml/ 3 cups and 220 ml/7 oz -- might need to drop


# Number of ingredients 
summary(recipesdat$n_ingredients)
table(recipesdat$n_ingredients)
#recipesdat %>% filter(n_ingredients ==84)

#not a recipe
#Asian market shopping list â€“ what I buy at Asian grocery stores!
#How to clean and cut a whole crab

#Number of steps -- majority say 0
summary(recipesdat$n_steps)
table(recipesdat$n_steps)
nstepszero<-recipesdat %>% filter(n_steps==0)

#Taking a look at cuisine
recipesdat %>% group_by(cuisine) %>% summarise(n = n()) %>%ungroup() %>%  arrange(desc(n)) # find the 5 most common and then have an other category
# Western, Caribbean, Asian

```


# Distributions of potential outcomes

```{r}
recipesdat %>% ggplot(aes(x = rating)) + geom_histogram(color = "black", fill = "white") + labs( x = "User Ratings", title = "Distribution of user ratings") 
# left skewed distribution- most recipes are highly rated -- 
# Users can rate a recipe from 1-5 stars. Looking at the distribution of ratings, we see all users rated recipes high ( median = 4.98).  
# -	Not many user ratings for some recipes
# -	People who follow the blogs enjoy the recipes and so they rate them highly

#Take a look at number of ratings
recipesdat %>% ggplot(aes(x = n_ratings)) + geom_histogram(color = "black", fill = "white", binwidth = 50) + labs( x = "Number of ratings", title = "Distribution of number of user ratings") 

summary(recipesdat$n_ratings) # 25 NA's

#recipesdat %>% filter(n_ratings >= 60) %>% dplyr::select(recipe,n_ratings, rating)
#plot(recipesdat$n_ratings, recipesdat$rating)
#think about removing the recipe with 2031 user ratings, and NA's
```


```{r}
#Remove missing and recipe with 2031
recipesdat<-recipesdat %>% filter(n_ratings != 2031) %>% drop_na(n_ratings) #1116
recipesdat %>% ggplot(aes(x = n_ratings)) + geom_histogram(color = "black", fill = "white", binwidth = 50) + labs( x = "Number of ratings", title = "Distribution of number of user ratings") 


#log transformed number of user ratings
#hist(log(recipesdat$n_ratings))

```

# New variables

```{r}
#Serving size categorical--MAYBE
#table(recipesdat$servings) # majority of recipes have serving size less than 10, we should consider using this as a categorical variable- based on the distribution we might consider levels: 1-4 servings, 5-8 servings, 9+ servings

recipesdat<-recipesdat %>% mutate(servingscat = as.factor(ifelse(servings >=1 & servings <=4,1,ifelse(servings >=5 & servings <=8,2,3))))
levels(recipesdat$servingscat)<-c("1-4", "5-8", "9+")
#table(recipesdat$servings)
#table(recipesdat$servingscat)
```



# Partition data 

We use `createDataPartition` to split the  data into equally-sized training and test sets. 

```{r message=FALSE, warning=FALSE}
set.seed(123)
recipes_index_train = createDataPartition(y = recipesdat$n_ratings, 
                                  times = 1, p = 0.5, list = FALSE)
recipes_train = slice(recipesdat, recipes_index_train)
recipes_test = slice(recipesdat, -recipes_index_train)

#checks of distribution
#summary(recipes_train$n_rating) 
#summary(recipes_test$n_rating) 
```

# Exploratory analysis

Using the training set we  make some plots to help you assess the relationship between our outcome of interest, `n_ratings`, and each of the other features in the dataset. 

```{r message=FALSE, warning=FALSE}
recipes_train %>% ggplot(aes(x = n_ratings)) + geom_histogram(color = "black", fill = "white", binwidth = 50) + labs( x = "Number of ratings", title = "Distribution of number of user ratings") 

recipes_train%>% 
  gather(predictor, value, c(cook_time, prep_time, servings, n_ingredients, n_steps)) %>% 
  ggplot(aes(x = value, y = n_ratings)) + 
  geom_point() + 
  facet_wrap(~ predictor, scales = 'free_x', 
             labeller = 
               as_labeller(c("cook_time" = "How long it takes to cook", 
                             "prep_time" = "How long it takes to prepare", 
                             "servings" = "Number of servings recipe yields",
                              "n_ingredients" = "Number of ingredients",
                               "n_steps" = "Number of total steps"))) + 
  xlab(NULL) + ylab("Number of User Ratings")



recipes_train %>% filter(!is.na(servings)) %>% ggplot(aes(servingscat, n_ratings, fill=n_ratings)) +
  geom_boxplot(alpha = 0.5) + 
  xlab("Serving size") + ylab("Number of User Ratings")
```


```{r message=FALSE, warning=FALSE}
library("PerformanceAnalytics")
my_data <-recipes_train %>% dplyr::select(n_ratings,cook_time, prep_time, servings, n_ingredients, n_steps)
chart.Correlation(my_data, histogram=TRUE, pch=19)

```


#  Random Forest -- might not work out

1) Conduct Random Forest regression to predict number of ratings or "popularity" of the recipe based on other features. 
2) Find which features are the best predictors
3) Evaluate accuracy of the model

```{r}
set.seed(123)

# Random forest
#First we fit a model with all predictors of interest
popularity<-randomForest(n_ratings ~  cook_time + prep_time + n_ingredients + servings + 
                           n_steps, mtry = 5, na.action = na.exclude, data = recipes_train)

popularity  # Does not perform well -- there is barely any corr between outcome and predictors

#Plotting the model will illustrate the error rate as we average across more trees and shows that our error rate stabilizes with around 200 trees but continues to decrease slowly until around 300 or so trees.
plot(popularity)


#Re-run model with only 200 trees
popularity1<-randomForest(n_ratings ~  cook_time + prep_time + n_ingredients + servings + 
                           n_steps, mtry = 5, ntree = 200, na.action = na.exclude, data = recipes_train)

popularity1


#Negative variance explained:
#http://developmentaldatascience.org/post/29-01-18_metaforest_no_effect/
#overfitting: If the dataset contains a large number of predictors that are uncorrelated to the outcome, the random forests algorithm will be forced to choose amongst only noise variables at many of its splits. This will lead to poor performance.

#https://stats.stackexchange.com/questions/447863/log-transforming-target-var-for-training-a-random-forest-regressor

#https://uc-r.github.io/random_forests


#How well does it perform on the test set?
preds<- predict(popularity1, newdata = recipes_train)
plot(preds, recipes_test$n_ratings) #that's not very good
abline(0,1)
mean((preds - recipes_test$n_ratings)^2, na.rm = TRUE)


#Variable importance
variable_importance <- importance(popularity1) 
varI_table <- data_frame(feature = rownames(variable_importance),
                  Gini = variable_importance[,1]) %>%
                  arrange(desc(Gini))
varI_table
#Mean Decrease Gini - Measure of variable importance based on the Gini impurity index used for the calculation of splits in trees.

#This is a fundamental outcome of the random forest and it shows, for each variable, how important it is in classifying the data. The Mean Decrease Accuracy plot expresses how much accuracy the model losses by excluding each variable. The more the accuracy suffers, the more important the variable is for the successful classification. The variables are presented from descending importance. The mean decrease in Gini coefficient is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. The higher the value of mean decrease accuracy or mean decrease Gini score, the higher the importance of the variable in the model.

#Based on these results, the most predictive variables are: cook_time and n_ingredients
#Re-run model with only these two predictors

popularity2<-randomForest(n_ratings ~  cook_time +  n_ingredients, ntree = 200, na.action = na.exclude, data = recipes_train)

popularity2


```




